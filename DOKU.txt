Doku: Anfangsziel war rudimentäres Spiel zu bauen um Aspekte zu testen und zu lernen. Das führte zu mehreren Iterationen besonders an der SteuerungBeim Auseinandersetzen mit MLAgents ist mir aufgefallen, dass die Art auf die Rewards in Reinforced Learning gegeben werden eine hohe Chance hat nicht kompatibel mit der Genre des Spiels zu sein, da
      die Rewards durch seperate Handlungen von Einheiten gegeben werden soll, zudem das Agenten Skript nicht natürlichweise direkten Zugriff hat. Sofern ist der Zyklus des Reinforced learning hierdurch bei dem "Reward" Schritt verzögert, falls Rewards außerhalb dieses Skripts gegeben werden sollen. Deshalb lag für mich hierauf der Fokus während der Einarbeitung und hierfür wurden auch die meisten Iterationen gemacht, um das auf irgendeine weise zu ermöglichen.
      Wichtig war außerdem die Benutzung und Einstellung der Konfigurationsdatei des Netzwerks. Das Training kann um einige größen effizienter sein, wenn man es richtig an seine Situation anpasst. Nicht zu vergessen Curriculum Training existiert.
      Steuerung ist sehr wichtig. Programmatisch ist die Steuerung für die AI nichts speziell anspruchsvolles. Für das Selbstspielen durch die Implementierung der Heuristic Funktion (welche blablabla...) merkt man aber direkt, dass mehrere bestimmte Befehle nicht direkt gegeben werden können. Man kann also im normalfall nicht direkt diagonal gehen, wenn man eine Steuerung implementiert hat
      welche die normale WASD Steuerung abbilden soll. Generell war ein wichtiger Aspekt, welcher öfter geändert wurde geprägt durch die Frage "Wie rudimentär kann oder muss ich dieses Feature machen, um das Verhalten durch den Agenten und seine Umgebung zu bekommen, die ich will".

22.06.2023 Die Wahl viel auf ein sehr bekanntes Spiel Konzept für das Thema, eine Tower Defense. Nicht ganz die allseits bekannte Art, sondern wie in Wintermaul Wars in WC III. Die Idee war zu Anfangs zwei Spieler zu haben, welche jeweils eine limitierte Fläche zum bauen haben und diese benutzen um Türme zu bauen, aber gleichzeitig auch Ressourcen verbrauchen um Einheiten gegen den Gegner los zuschicken. Das Ziel war die Einheiten bis in die gegnerische Basis zu senden, wo sie Lebenspunkte abziehen sollen, bis dieser 0 Lebenspunkte hat. Das Layout und der Mix aus verschiedenen Turm Typen ist wichtig für die Defensive. Sowohl auch eine Balance zwischen senden versch. Einheiten
und bauen der Türme. Aber zwei KI zu haben welche auf "verschiedenene" Konzepte (Angriff und Defensive) gleichzeitig trainiert werden, hat sich auf weiteren Blick als zu komplex für diese Arbeit herausgestellt. Die Wahl viel auf ein Asymmetry, bei der ein Spieler die Defensive hat und der andere die Offensive (Monster schicken). So ist hier noch ausreichend Komplexität zu finden um die KI zu trainieren gegeneinander zu spielen und sich zu verbessern. Die Höhe der Komplexität die man hier erreichen kann ist beliebig und wird je nach Eigenresultat angepasst.    
Wichtig gegen Anfang war das richtige Layout des Tower Defense spiels zu finden -> hier anlehnung and Wintermaul Wars und ähnliche "Fun Games" im Spiel Warcraft III. Ziel war es Ein  anfangs simples Spielfeld zu haben, das nach Bedarf erweitert werden kann, so muss also der Code gut und schnell anpassbar sein. Hier war ein sehr gut anpassbares Grid System erforderlich, um eine eventuell große und Komplexe Anzahl an Informationen zu speichern.
      Dies wird nicht nur bei dem Platzieren der Türme wichtig, sondern auch für das Pathfinding der Monster und weiter in die Zukunft geschaut, für das Logging und visualisieren und Daten.

23.06.2023 mehr Modularität innerhalb der Grid Cells durch einfügen komplexer Objekte als Referenz Punkt für eine Zelle. Entscheidung für Einfaches Targetting, nicht Physiks based bei Projektilen (for now)

28.06.2023  Umschwung auf Unity TD Beispiel anstelle von Eigenentwickelten Prototypen. Vorteile : Großteil der gewollten Funktionen sind vorhanden - Modularität vorhanden - hohe Qualität des Beispiels in Struktur des Codes; Nachteil: Relativ anspruchsvoll zum Einlesen und spezielles umbauen von Strukturen nach späterem Bedarf -> Fazit: Zeitvergleich von Einlesen zu Eigenentwicklung ungewiss, aber Funktionalitäten vorhanden und überschaubar und hochqualitativ und deswegen wird diese Methode verwendet.

30.06.2023 Lange Debug Zeit da ich mich durch die Call Hierarchy durchhangeln muss um Bugs/Fehler zu finden in meinen Skripts, da ich funktionen aus ihrem Kontext reiße und im Agenten Skript aufrufe. Nicht direkte Dependencies wirken gegen mich, manche Funktionen sind zu tief in der Hierarchy, also muss man für die Funktionalität welche aufrufen die höher sind -> den Effekt der Funktionen und die Dependencies zu finden dauert Zeit.




04.07.2023 Alle Resets der Szene durch den Action delegate machen (homeBaseDestroyed in LevelManager) der von allen Notwendigen Funktionen der Funktionalitäten subscribed wird. Anfügen von kleinen Funktionen wenn nötig.
05.07.2023 Je nach Gewinn oder Verlust entsprechende delegates aufrufen, welche die entsprechenden Rewardfunktionen im Agentskript aufrufen, welche den resetAll delegate (vorher homeBaseDestroyed delegate) aufrufen.



10.07.2023 Switch von propriäterem Timer auf C# Timer class, propriäterer Timer war zu unübersichtlich und teilweise fraglich benutzt
11.07.2023 C# Timer Benutzung nicht möglich, da threaded und get_gameObject methoden nicht aufrufbar in Threads (was benutzt wurde -> wäre zu unnötig komplex gewesen das zu versuchen zu ändern)
	   jetzt einfach TimedWave weggelassen und Wave nur benutzt
	   Angefangen mit Observationen von TowerBuild coordinates, Tower type and Currency. Eventuelles Problem mit Aufrufreihenfolge, da die variables in BuildTower assigned werden, aber in CollectObservations abgerufen (needs to be tested)


13.07.2023 Setup der Szene für Training optimiert -> kleineres Grid und ausstellen der Animationen und Sounds. anfänglich lange Testläufe ~30min - ~1h30min





18.07.2023 StreamWriter klasse geschrieben um Observations zu überprüfen, hat zu viel lag sobald Funktion aufgerufen wird davon, daher aus Eis bisher da nicht notwendig - weiterhin wahrscheinlich weitere Vereinfachung der Szene notwendig zum Trainieren, da std. dev. nicht abnimmt und Rew. nicht konstant zunimmt



21.07.2023 - baseHealth für Training verändert - keine gute Änderung bisher, aber auf einem richtigen Pfad, Training hat anders ausgesehen in der Statistik



24.07.2023 - Änderung verschiedener Config Parameter zum testen

26.07.2023 - SAC als trainertyp versucht, aber keine sichtlichen Änderungen im Training - Viele Freezes während SAC training, dadurch sehr sehr langsames Training
27.06.2023 - Bis nächstes Meeting Anfang mit AttackAgent und Umbau der Code Struktur für Units senden und alles, dass damit zusammenhängt



01.08.2023 - Tower Platzierung war falsch, hatte Türme in die Liste eingetragen und observiert, auch wenn es auf einem besetzten platz war, oder der Turm durch Currency Mangel nicht gebaut werden konnte (trotzdem kein besseres Ergebnis) - Dafür Meeting mit Manu
	   - Random Türme aufsetzen und AI Turmtyp entscheiden lassen hatte fragwürdige Erfolge. Sehr hoher average reward direkt zu anfang und bei 1,00 average reward ist es immer mal wieder auf dieselbe Zahl zurückgefallen bei der Summary anstatt 1,00. Needs more experimenting
02.08.2023 - fixed gridtilenumber 0 being represented twice by x0,y0 and x1,y0 = Kein Ping Pong der Werte mehr (1 Versuch bisher), aber hört relativ schnell auf sich zu verbessern
03.08.2023 - Epsilon von 0.2 auf 0.1 num_layer von 2 auf 3, learning_rate von 0.0003 auf 0.0001 erzielt schlechtere Resultate
	   - learning_rate: 0.0002, epsilon: 0.3,  hidden_units: 256, num_layer: 3 - Result: Deviation bis ~ 0.77, Mean bis ~0.63 und springt danach wieder hoch
	   - learning_rate: 0.0001, epsilon: 0.15 - bis 60k Steps, um runtergegangen bis ~0.8 dann wieder hoch und runter
04.08.2023 - standard config geht bei 0.6 (deviation) ~0.78 mean, wieder hoch
	   - beta: 0.005, time_horizon: 128, no curiosity, num_layers: 2, geht von 0.7 runter auf 0.66 und wieder hoch zu 0.77-0.8
	   - added back curiosity: 0.01, beta: 0.01


08.08.2023 - learning_rate: 0.0002, beta: 0.03 - results: ~value either ~7.5 or every other value ~6.5
	   - beta: 0.01 - results: werte zw. 0.65 und 7.5 aber inkonsistenter
	   - time_horizon: 64, hidden_units: 256, learning_rate: 0.0003, results: pendelt sich um ~0.6 - ~0.7

09.08.2023 - After change (s. Zeitplan) and only Tower Select Training, Avg. is ~8.6 somewhat consistent starting at 1000000+ steps, stopped on 2mil steps with 